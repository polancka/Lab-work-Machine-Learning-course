#Lab 2
#resampling methods for model evaluation and attribute selection.
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import ipywidgets as widgets
import random as rnd
import pandas as pd
import math
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from scipy.stats import bootstrap
import random 

##function leave_one_out preforms a LOOCV (leave-one-out cross validation) on give data
## and returns the MSE (Mean squared error)
from scipy.stats import bootstrap
import random 

##function leave_one_out preforms a LOOCV (leave-one-out cross validation) on give data
## and returns the MSE (Mean squared error)

def leave_one_out(data):
    MSEs = 0
    for i in range(len(data)): #in range(len(data))
        #split into train and test, test is just one example, split into x and y

        train_x = np.array(data.drop(data.iloc[[i]].index).drop(columns= 'ViolentCrimesPerPop'))
        train_y = np.array(data[['ViolentCrimesPerPop']].drop(data.iloc[[i]].index))
        train_y = train_y.reshape(-1,1)

        test_x = np.array(data.drop(columns= 'ViolentCrimesPerPop').iloc[i]).reshape(1,-1)
        test_y = np.array(data.iloc[i][['ViolentCrimesPerPop']])
        test_y = test_y.reshape(-1,1)

        #train the model
        curr_model = LinearRegression().fit(train_x,train_y)
        curr_fit = curr_model.predict(test_x) 

        #calculate the MSE (mean squared error) for test example and save it 
        curr_mse = math.pow(test_y[0] - curr_fit,2)
        MSEs += curr_mse

    leave_one_out_MSE = MSEs/len(data)
    return leave_one_out_MSE

## the k_fold function preforms k-fold cross validation and returns overall MSE and overall r^2 score

def k_fold(x,y, k):
    iterations = len(data) // k
    overall_mse = 0
    overall_r2 = 0
    for i in range(iterations):
        start_index = i*10
        end_index = start_index + 9

        train_x = np.array(x.drop(x.index[start_index:end_index+1]))
        train_y = np.array(y.drop(y.index[start_index:end_index+1]))
    
        test_x = np.array(x.iloc[start_index:end_index])
        test_y = np.array(y.iloc[start_index:end_index])
        test_y = test_y.reshape(-1,1)

        fold_model = LinearRegression().fit(train_x,train_y)
        fold_predicts = fold_model.predict(test_x)
    
        fold_mse = mean_squared_error(test_y,fold_predicts)
        current_r2_score = r2_score(test_y, fold_predicts)

        overall_r2 += current_r2_score
        overall_mse += fold_mse
      
    overall_mse = overall_mse / iterations
    return overall_r2,overall_mse

##function forward attribute selection gradually builds the best model. First models are build for each attribute. The best is 
## added to the final model. The attributes are added one by one, based on the r^2 score of their model. The r^2 score of 
## the model we are buidling is saved on every itteration. The plot is saved in plots/lab2_r2. Current implementation does the 
## attribute selection with 10-fold cross validation. Based on the r^2 score, the final selected attributes are: 
## 'RentLowQ',
#  'MedOwnCostPctIncNoMtg',
#  'PctLess9thGrade',
#  'LemasPctOfficDrugUn',
#  'pctWWage',
#  'PctWorkMomYoungKids',
#  'RentHighQ',
#  'racepctblack',
#  'PctPersDenseHous',
#  'numbUrban',
#  'NumUnderPov',
#  'PctIlleg',
#  'NumStreet',
#  'MalePctDivorce',
#  'PctWorkMom',
#  'pctUrban',
#  'HousVacant',
#  'racePctWhite',
#  'PctKids2Par',
#  'ViolentCrimesPerPop'

def forward_attribute_selection(x, y):
    ## za vsak atribut najprej model samo s tem atributom, poglej kako napoveduje y
    ##tisti ki njaboljše napoveduje (najmanjša MSE) dodamo v model
    ## ponavljamo dodajanje dokler je to smiselno
    r2_scores_and_indices = list()

    
    for i in range(100):
        x_subset = x.iloc[:,i]
        current_r2_score, current_mse = k_fold(x,y,10)
        r2_scores_and_indices.append((i,current_mse))
   
    r2_scores_and_indices.sort(key = lambda x: x[1],reverse=True)

    list_of_indexes = list()
    ploting_r2 = list()
  



    for i in range(1,len(r2_scores_and_indices)):
        list_of_indexes.append(r2_scores_and_indices[i][0])
        
        x_sub = x.iloc[:,list_of_indexes]
        new_r2_score, current_mse = k_fold(x_sub,y,10)

    
        ploting_r2.append(new_r2_score)

    plt.plot(ploting_r2)
    plt.show()
        
        
    return 1


##bootstraping function, that takes random samples from the data set. On each random test set, it trains a model,
## and calculates the r^2 score fot that model. The scores are stored and shown on a histogram. 

def bootstrap_samples_and_model(whole_train, test_x, test_y, n):
    r2_scores_of_models = list()
    overall_r2 = 0
    
    for i in range(1000):
         # outer loop, builds the model
        train_set_x = []
        train_set_y = []
        for j in range(len(whole_train[0])):
             # inner loop builds train set with resampling (bootstraping)
             current_row = random.randint(0,len(whole_train[0])-1)

             train_set_x.append(np.array(whole_train[0].iloc[current_row]))
             train_set_y.append(np.array(whole_train[1].iloc[current_row]))

        # print(train_set_x)
        # print(train_set_y[1])

        #build a model and calculate r^2 score
        current_model = LinearRegression().fit(train_set_x,train_set_y)
        fitted_values = current_model.predict(test_x.values)
        curr_r2_score = r2_score(test_y,fitted_values)
        overall_r2 += curr_r2_score
        r2_scores_of_models.append(curr_r2_score)


    overall_r2_score = overall_r2/1000
    return overall_r2_score, r2_scores_of_models



        

##------------------------------------------------------------------------------------------------
#some function calls are commented because it takes around several minutes to run them.

#Download the "Communities and Crime" dataset and prepare the data so that you will be able to use them for linear regression.

# Reading the CSV file
data = pd.read_csv('data/communities+and+crime/communities.data', na_values='?')

# Data Manipulation
data = data.drop(['state', 'county', 'community', 'communityname', 'fold'], axis=1)
data = data.sample(frac=1, random_state=3)

#delete the columns with a lot of null values and the remaining rows with a few null values
data = data.dropna(axis=1, thresh= 1500)
data = data.dropna(axis = 0)
#print(data.info)  [1891 rows x 101 columns]

#into x and y sets
x  = data.drop(columns= 'ViolentCrimesPerPop')
y = data[['ViolentCrimesPerPop']]

# #Implement the cross-validation method and the leave-one-out method.
# loo_MSE = leave_one_out(data)
# #print(loo_MSE) #0.018644326733171977
# k = 10
# k_fold = k_fold(x,y,k)
# print(k_fold) #0.018313116621930674


#Implement forward attribute selection. Fit linear regression.Use the attribute selection method with the implemented cross-validation to select a reasonable set of attributes for your linear model.
#which metric and the criteria
trainX, testX, trainY, testY = train_test_split(x, y, test_size= 0.3, random_state= 42)
#attributes = forward_attribute_selection(x,y)


#Test your model and report the results.

data_few = data.loc[:, ['RentLowQ',
 'MedOwnCostPctIncNoMtg',
 'PctLess9thGrade',
 'LemasPctOfficDrugUn',
 'pctWWage',
 'PctWorkMomYoungKids',
 'RentHighQ',
 'racepctblack',
 'PctPersDenseHous',
 'numbUrban',
 'NumUnderPov',
 'PctIlleg',
 'NumStreet',
 'MalePctDivorce',
 'PctWorkMom',
 'pctUrban',
 'HousVacant',
 'racePctWhite',
 'PctKids2Par',
 'ViolentCrimesPerPop']] #make a selection only a few attributes

new_x  = data_few.drop(columns= 'ViolentCrimesPerPop')
new_y = data_few[['ViolentCrimesPerPop']]

#Implement the bootstrap method and apply it to the train set to generate 1000 different train sets and train 1000 different linear models.
trainX_b, testX_b, trainY_b, testY_b = train_test_split(x, y, test_size= 0.3, random_state= 42)
train_whole = [trainX_b, trainY_b]


mean_r2, list_r2s = bootstrap_samples_and_model(train_whole, testX_b, testY_b, 2)

# ##[0.6321187533486513, 0.6289904714788453, 0.6216109342255682, 0.6243599622784604, 0.6048775163282203, 0.6386425565220342, 0.6066849161673517, 0.6387298644101977, 0.6384298496123115, 0.6329786173281935, 0.6264915335236197, 0.6238966140447897, 0.6413539200232982, 0.6150527836711721, 0.6125041404741978, 0.6074901366271516, 0.6081837098536957, 0.6042273953563899, 0.62189413150462, 0.6130408531553573, 0.6368755121984766, 0.6153108508947909, 0.623786042043925, 0.6190271881549647, 0.6365538010273346, 0.6070584549621636, 0.5788655463170913, 0.6131189544201767, 0.6261575321512411, 0.6235922835101515, 0.6218026254216049, 0.6176927419618834, 0.6367802916027434, 0.6241706499970956, 0.6203075555427868, 0.6144590443674154, 0.6348467726867163, 0.6206026091621235, 0.6232203135470853, 0.6093389334472921, 0.6051473763294646, 0.6251884452203268, 0.6278774471560775, 0.6423554695394113, 0.6233619858470179, 0.5950876781844727, 0.6190479928742508, 0.6003550258638541, 0.6325572079139253, 0.6248481094714768, 0.6300322546441796, 0.6244206750155579, 0.6478157387375321, 0.606332012695634, 0.6258802152078009, 0.6293754076728106, 0.6247949390175371, 0.6117864769462492, 0.6401449909931961, 0.6079914527261694, 0.5887431051334696, 0.6155770897618476, 0.6370923021424006, 0.614903136996338, 0.5811898837735926, 0.6251647817780273, 0.6276331857968964, 0.6185166713191314, 0.6229185249759868, 0.6140661901381601, 0.5967170354014668, 0.630920459087468, 0.6496110491202403, 0.6231545658155775, 0.635924812248232, 0.5885424792234193, 0.6096614480501464, 0.5927312665903758, 0.6194750512944449, 0.6387368292565967, 0.6218505046214226, 0.6151374226063966, 0.6245866312337092, 0.6319566027673427, 0.6517433728593223, 0.6300690585046782, 0.6027957655035351, 0.6310508271561739, 0.6143574794706752, 0.6175760029674024, 0.6156894799921939, 0.6238906837777076, 0.6179044153397641, 0.6299303485879597, 0.5849262559468686, 0.6286235372964123, 0.5984253770923541, 0.6418451993253084, 0.5939794854067861, 0.6415994603573525, 0.6160308532739053, 0.6152986235167869, 0.6088858911567665, 0.6098964363635863, 0.6088076137307339, 0.6399823447430082, 0.6283572966714215, 0.6301239947458542, 0.6195392760198813, 0.6245037431302821, 0.5911359326380803, 0.6435100289887317, 0.6367750609902691, 0.6256032979176458, 0.6282613125910586, 0.6056344481361968, 0.6299987464511282, 0.6051640516259833, 0.6266921627551608, 0.6201887834298188, 0.6298304133723132, 0.622333757678606, 0.6278342825534802, 0.6203933753815738, 0.6307412823668985, 0.6093578777931373, 0.610344056608686, 0.6247941489795468, 0.636067719585846, 0.6074700735976768, 0.6235814847044705, 0.6297659629805488, 0.6194413121365917, 0.5923462331758969, 0.6145400505053382, 0.6298076899899424, 0.6064235197235281, 0.6306013049998254, 0.6331778820792893, 0.6316632600457929, 0.6396521869261855, 0.5980352124251835, 
# 0.6090984281303033, 0.6248880281260116, 0.5850113424655821, 0.632047714283904, 0.6284900769229487, 0.6361576101868269, 0.6234401052189407, 0.6226378552610017, 0.6347943036597969, 0.6199776826803405, 0.6071613753981969, 0.6300597230140872, 0.6180718408361199, 0.6438179667976702, 0.6246653532567843, 0.6242867806375809, 0.624200276387904, 0.6148838467881685, 0.6315838884481866, 0.5996578688210124, 0.6338425361572122, 0.6115689284501333, 0.6037262960542897, 0.627308431941273, 0.648539105091738, 0.6087623468647779, 0.5962412810650768, 0.6150065357390246, 0.6141014481877347, 0.6338257796721021, 
# 0.6280805474177295, 0.611944968997457, 0.6239998377624063, 0.6365231818338852, 0.6366817074853859, 0.625926687934198, 0.6138788062034891, 0.6303949293739686, 0.6339149864063496, 0.6070909049119099, 0.6170319678302301, 0.6152233532123149, 0.6277025089719421, 0.6139309778776391, 0.6155568330315593, 
# 0.6208781187184567, 0.6108676960431682, 0.6235706580693584, 0.6498861630991228, 0.6391203548790163, 0.6136345309341764, 0.5968249144849005, 0.6049762064365247, 0.6256678191090895, 0.6065294820423008, 0.625959298019908, 0.645966496951146, 0.6324574353792913, 0.6341198517192268, 0.6176421262842404, 
# 0.6336704531981507, 0.6116869568486785, 0.6299352209212689, 0.6138162828239278, 0.5911726936491069, 0.6203533347010599, 0.5901286413861317, 0.5967377578624791, 0.635881987671507, 0.6374616914090601, 0.6187863871865218, 0.6132902592208136, 0.6167240276886198, 0.6016117968349433, 0.6317140694694264, 0.613562946102169, 0.6307340877283842, 0.6284270623068533, 0.6122364556757682, 0.6166031649085668, 0.6220361168539652, 0.6199430301020947, 0.6045798699792944, 0.6125160980981852, 0.6288633500369678, 0.6153407408193436, 0.632472077100811, 0.6234735490428839, 0.6486482897717603, 0.5969806697403381, 0.5950904694606846, 0.6278655399079918, 0.599494599146054, 0.6312747839128843, 0.5949000992521611, 0.6337605360137889, 0.6296374327120808, 0.6165785948614702, 0.6069963527119975, 0.6226462863001962, 0.6216810599255791, 0.6339362321912472, 0.6454520921495223, 0.633176885634868, 0.6165367649372613, 0.6306317904337282, 0.6080540150636886, 0.6381989487244337, 0.6235829276954141, 0.6278138670939819, 0.6199691442262654, 0.6366457083691721, 0.6122679251895422, 0.6388080062436247, 0.5984506222114827, 0.6112065079158935, 0.6163848036409325, 0.6030816427472946, 0.6168839099556565, 0.6282475111951557, 0.6325612945690908, 0.6348803486636596, 0.6388954136383858, 0.6341194174730967, 0.6057055446644688, 0.609929169901729, 0.6130621443863015, 0.6077363996645113, 0.6282865277069452, 0.6243480429808879, 0.6019903320960979, 0.6377118982571821, 0.602349736570251, 0.612369159960443, 0.5972316773687002, 0.6072927468112089, 0.5983821828705214, 0.6367641597228031, 0.6232357298774294, 0.62923229276691, 0.6024141005736787, 0.6341946594906, 0.6174945360746428, 0.6209378999783116, 0.6321385403506335, 0.6138010238410234, 0.6354552192658428, 0.6464816907480748, 0.631372626361189, 0.6437919896439721, 0.6311897332557499, 0.6190480290547042, 0.621135108231528, 0.6153606361473032, 0.6140344875906367, 0.6067250701873912, 0.6109134976963311, 0.6193034868827858, 0.6215944442002305, 0.6452106197211496, 0.626084574825007, 0.6391448430228743, 0.6292542104939451, 0.6168439078843607, 0.6174848744866894, 0.6366003418077502, 0.6048274465760328, 0.5920226078392481, 0.6201690984250416, 0.6216222683590251, 0.6281321059286786, 0.6502560092164603, 0.6165168486884329, 0.6204722297374456, 0.6047143845234608, 0.621886316086345, 0.6252012515522107, 0.6057674854097195, 0.6352767313119507, 0.6166356807422857, 0.6165640873293323, 0.629264724862362, 0.6450816772723335, 0.622022057450746, 0.6247722643015587, 0.6378575758102096, 0.6193873112749188, 0.5872955010093707, 0.639335528490409, 0.6112203986045881, 0.6210028129336582, 0.62098571992361, 0.6271780034714178, 0.6293637611010772, 0.6035201951160326, 0.6503473444197783, 0.6360984604294757, 0.5843464846536428, 0.621482213625256, 0.6252676139293328, 0.6270463753640472, 0.6080982409026875, 0.6283782862903116, 0.6155331108514954, 0.61746595060521, 0.5741416700614415, 0.638246054695701, 0.5957472248237401, 0.6401512498492409, 0.6295655641124468, 0.6135991867450238, 0.611878377901876, 0.6032646069582595, 0.5903354907347664, 0.642860717141509, 0.5932294321614225, 0.628224248336446, 0.601622583028007, 0.6361840149910644, 0.5771506964675428, 0.5955712021265969, 0.6081253630270637, 0.6253580035675796, 0.6244857741822374, 0.6171199007586399, 0.6325760141080194, 0.6127148691633486, 0.6125853117917339, 0.6010998038866394, 0.6455846245880184, 0.6207455065233647, 0.6314305841514305, 0.6375769867986034, 0.6210567953270968, 0.6265146101079221, 0.6098561115550517, 0.6066115834815371, 0.642093747992327, 0.6277216895861389, 0.617377047349075, 0.6187532523911488, 0.6255929227198429, 0.6383653228814827, 0.6454468092955932, 0.6133052072144493, 0.5996591732290724, 0.6278062318980862, 0.6325527564968563, 0.621348248579289, 0.6099253622713358, 0.59778267962562, 0.6109643498060919, 0.619898184342814, 0.6201257990058564, 0.6270473352305853, 0.6292043831785519, 0.6269819434082057, 0.6166704308210922, 0.6159746172727423, 0.6098451603246247, 0.6237600645523642, 0.6006630986009015, 0.6172527993679171, 0.6076584790875048, 0.6187217896077536, 0.6322742936402392, 0.6122587310562714, 0.6334319925618217, 0.603481135164627, 0.6250586742491135, 0.6206804614184639, 0.6170775820129153, 0.6126876932542007, 0.6153767785422607, 0.634992579608133, 0.61828299347807, 0.6362808467106172, 0.633426741764832, 0.626198322217834, 0.6039887591305687, 0.6320078555195201, 0.62480192575712, 0.6086471403624555, 0.6215203555495448, 0.6246744781559878, 0.5892258073396395, 0.604163027658503, 0.6081135620865457, 0.6238286080872439, 0.6246655573293884, 0.6304369291376292, 0.6305385197697766, 0.6285623223834811, 0.6214372275951672, 0.6091129589886008, 0.6356424868560941, 0.6086790875630566, 0.6045102893038192, 0.6142306520407459, 0.6038470296693466, 0.5833531798992816, 0.6143713151658345, 0.6175176640616094, 0.6283848408228269, 0.6145586639327861, 0.6213598260105442, 0.6106559238623066, 0.6401026933359486, 0.6011277387742731, 0.6309511353040687, 0.6171346972296612, 0.6325631349893801, 0.6183374972322676, 0.627705056772804, 0.6288525820382346, 0.6151259535485154, 0.6257071992021824, 0.609665811148221, 0.6225362558628338, 0.6138911403392618, 0.6236586288811522, 0.6335817087752686, 0.6305327159037843, 0.5954425262917892, 0.6274331138611416, 0.5918422756660733, 0.6271077520824135, 0.6032096962901533, 0.6033048134667827, 0.6266083156683999, 0.616918555754527, 0.6377169356312803, 0.6025265474490011, 0.6377291345819638, 0.6259562281286415, 0.6397630151700326, 0.6182184229090237, 0.630162014276821, 0.6106298200487492, 0.6067920140679599, 0.6120677718621208, 0.6489409330481644, 0.6334054651330534, 0.6195131315952985, 0.6380798785352914, 0.6159803706953156, 0.6370778886195652, 0.6082695787679342, 0.6174933613316086, 0.6406948377937167, 0.6275141222610257, 0.6132368270676148, 0.6322500920864167, 
# 0.6436429531303296, 0.6049799025680688, 0.6321485035947185, 0.6223067722332425, 0.6182879753550643, 0.6340439591869254, 0.6090807144524526, 0.6190892022993297, 0.6052268549135471, 0.6318497537660079, 0.6316822379998787, 0.6140617862539101, 0.6158969777166764, 0.630013591691328, 0.6122309995792438, 0.6226633464835695, 0.6188812449772703, 0.6294204012851189, 0.6244720075269328, 0.593645535223368, 0.6320236409754234, 0.6084122107690015, 0.6239869753009237, 0.6272014322759246, 0.6234560566014574, 0.6336145740784416, 0.6449710413444678, 0.6101908186803118, 0.6283761196084432, 0.6172640356788811, 0.5969583631391437, 0.6389805270886892, 0.61320512560583, 0.6265238040655223, 0.6287714561638222, 0.6255808708895372, 0.6418920715051992, 0.6367501125832844, 0.6054649763316639, 0.6418130163590363, 0.6209068820992485, 0.6226747394599288, 0.6425215992000599, 0.6124119785638436, 0.622889130013083, 0.6268391948620444, 0.6263035379346326, 0.632869711971005, 0.6184818541695434, 0.6512956199521979, 0.6207538273409465, 0.6110530765495306, 0.5852118525944153, 0.6089495297276746, 0.6087795194018788, 0.6237751177290835, 0.5945023246038398, 0.6189774500606169, 0.6165122351190219, 0.6310891425862897, 0.6114376147278697, 0.6110031234606417, 0.6278758643862921, 0.633515978871005, 0.6367337253127007, 0.6193073733258475, 0.6364371285881502, 0.6250680732351607, 0.6301963175220655, 0.6305624061930697, 0.6228692288653879, 0.5978472508549196, 0.6341284494259092, 0.6394248751991852, 0.6003255906160763, 0.6209159479831459, 0.6289129776550233, 0.6258503403494868, 0.6206106519066412, 0.61307961484444, 0.6357131116908228, 0.6284860382803973, 0.6325274941784154, 0.6282326170533108, 0.6385830650458598, 0.6067760170271497, 0.6073352687683635, 0.635716552466518, 0.639241570187983, 0.6429132260398016, 0.6302653343418083, 0.6126617726970922, 0.621545489458927, 0.6216837572360099, 0.6351345903454257, 0.6212854390080229, 0.6137493836344128, 0.6206411616311038, 0.6238838308507626, 0.601719990353641, 0.6235417793890847, 0.60111504723998, 0.6342664234318592, 0.6193738012156356, 0.624801078886503, 0.6254625586164558, 0.6469073295177574, 0.6136733648044332, 0.6075107626297602, 0.5986913886710185, 0.6051903188947674, 0.6183218906170636, 0.6180320787476121, 0.6242421631848423, 0.6320614440424441, 0.6090093858722406, 0.6350435448302214, 0.6415233926589243, 0.6145617817289123, 0.6371741796663617, 
# 0.5973196228170594, 0.6282701160563147, 0.6245311249122424, 0.634376179912955, 0.6303216261709761, 0.6209627661587875, 0.6281617958596207, 0.6326502235252023, 0.6210528256018768, 0.6239033908428127, 0.6345908301954354, 0.6222186896560526, 0.6203444536175021, 0.6161410018365163, 0.6123897746101122, 0.6178024597652918, 0.6203218186164003, 0.6214716722210849, 0.633468534213937, 0.6028308382136887, 0.600077730870523, 0.6443436163329487, 0.6299087365180847, 0.603043532654314, 0.6321666484302718, 0.6175113539009305, 0.6093887045723505, 0.609714385245899, 0.6283953196229699, 0.6185761251757534, 0.6297120979999846, 0.6029979176071005, 0.6223795855908285, 0.6237836131457244, 0.6254935245057722, 0.6399470606323909, 0.6167032459259039, 0.613697052060824, 0.6085699777054503, 0.6337069347116311, 0.6132506398041403, 0.6257715280697866, 0.6263373440695981, 0.6098194659827552, 0.6218554309972987, 
# 0.6187866012235808, 0.630601005667158, 0.5932178473129464, 0.6428143526540471, 0.6333110229131459, 0.6012793984387494, 0.633215782322186, 0.6274657618945447, 0.6078719650655473, 0.6073616716410564, 0.5809673443775192, 0.6164443273725906, 0.6055965977491725, 0.6202676911218175, 0.6231004820841989, 
# 0.633253709051609, 0.6065705865149892, 0.6260815765527806, 0.6134530910861551, 0.6157333929206594, 0.6172898195922829, 0.6018457982267044, 0.6143547036584478, 0.6167358978519117, 0.6069483748355933, 0.6300861074798183, 0.6266494278871259, 0.6351831334879039, 0.6224528762625987, 0.6236978264399144, 0.6161178274378322, 0.622571861479026, 0.6237942796700573, 0.6074169333248409, 0.6025874916010359, 0.6301006530487971, 0.6270639452057519, 0.5810616227032404, 0.6192266348286253, 0.6358408975589716, 0.6297273271365208, 0.6309252225000695, 0.6207785219133566, 0.5914213174186784, 0.6167871197679489, 0.6030613394993711, 0.6425835617570776, 0.6186642157858063, 0.6418976949704952, 0.6066509973105954, 0.6121891158581023, 0.6160220503153021, 0.6200807686352602, 0.6195843082628025, 0.6077875405886098, 0.6290236598573178, 0.6416255680646794, 0.615331832761568, 0.6231086540657544, 0.6196031087813716, 0.6288517749415554, 0.601199215927024, 0.6309897705851856, 0.6467327056552143, 0.6218151524733245, 0.6212341043316739, 0.5959097089926146, 0.6280973610186027, 0.6255270742709712, 0.6198292106534526, 0.6100217368953544, 0.6334026403711233, 0.603916587958108, 0.6006719366830597, 0.6304181796854914, 0.6058624683382132, 0.6167999994132131, 0.6418755234737525, 0.6104256093573628, 0.6105971424355763, 0.6414264898443766, 0.6227126808851509, 0.6118584144458887, 0.6201246162787619, 0.6310772491355003, 0.6216612477668758, 0.6204425189494487, 0.6008565781118729, 0.615958692251436, 0.5933932209997963, 0.6090572009297138, 0.6091361322184353, 0.6045081413365889, 0.633213041921991, 0.6062947245616652, 0.6428026627379315, 0.6067915475683463, 0.6220427052374696, 0.6157753759055189, 0.6209098400048093, 0.6092563335364068, 0.6095794907864919, 0.5940587074124821, 0.6257732688540878, 0.6380899645335387, 0.6237913924494819, 0.6220344814207106, 0.6377178417531356, 0.6221342325026815, 0.6182028625406855, 0.630524690818834, 0.617955564332201, 0.6124778206937798, 0.6351633851633909, 0.6260970134695374, 0.6436566058985935, 0.6276386081205109, 0.6348072913503973, 0.6043761270666042, 0.6263835010465132, 0.5852592650153431, 0.6255488014329477, 0.6300162065954679, 0.6146523279202916, 0.6243295796585873, 0.6214293923014225, 0.6175171119715641, 0.617717714557708, 0.6252580982788142, 0.6304444077096449, 0.6423375180137406, 0.5976026074835115, 0.6248586549654103, 0.6256903145044503, 0.6367104370444293, 0.6106815304144633, 0.603768878441822, 0.6390483507615743, 0.6256618095725006, 0.6219662639891186, 0.6194715830338277, 0.6236882933626249, 0.6363129788078774, 0.622316707396088, 0.5905073501640841, 0.6299296827082359, 0.6377394594723298, 0.6182799746493941, 0.6303481306411156, 0.6116750393920696, 0.6404111740274372, 0.6081984390833015, 0.6121647363735782, 0.631036974171428, 0.5705371040904416, 0.6124794459538165, 0.610327507362092, 0.6257998461411832, 0.6362081328721589, 0.6280813939905789, 0.6072561255768393, 0.6000518840825495, 0.6258033637844984, 0.6398329455104848, 0.6133141521896476, 0.6204807479590286, 0.6268663226928968, 0.6414894260152918, 0.6129343506893765, 0.6407896978660184, 0.6007774108840682, 0.5981174561742004, 0.617178467761319, 0.6204206650047845, 0.6217422524727942, 0.5987807984809191, 0.5988668453088264, 0.6005445072278224, 0.6219921765076291, 0.618901100529654, 0.6007864826526799, 0.6110805928569862, 0.6184526099669563, 0.6241706738973711, 0.6319614799277845, 0.6252413237809558, 0.6090846562473975, 0.6030969371146331, 0.6099838477240026, 0.6103353057291145, 0.609362630074171, 0.6230657265612324, 0.6232592956360901, 0.614629303268647, 0.6283304826442415, 0.6220268082453777, 0.6207049683032002, 0.6106149530840745, 0.6066586109526106, 0.6265160549244569, 0.6263825585652927, 0.6347419095426119, 0.6335951542261908, 0.6344804202896996, 0.6357064176354569, 0.6249752424341315, 0.6108046640568703, 0.6180927170254045, 0.6017705275299798, 0.6350053567046361, 0.6270461233474969, 0.6381850758668048, 0.6249883192739045, 0.6177792529713253, 0.6326607220940907, 0.6167954925244239, 0.6129735149317916, 0.6201168060472744, 0.613319434057542, 0.6254010116870599, 0.6162566705886083, 0.626496015969014, 0.6330631495536674, 0.6293997822654318, 0.6214283224131487038722918, 0.6163522988943847, 0.6117668254151634, 0.6239048433536347, 0.6336530142128105, 0.6316169056100034, 0.6244599680029059, 0.6355306649802651, 0.6175575625672525, 0.6246206892545871, 0.5989958317568599, 0.6339197011861439, 0.6372716189801131, 0.6294083151210752, 0.6240759957806516, 0.6207933242781041, 0.6064822072941769, 0.6275278118772825, 0.6304333414466806, 0.6094065073481391, 0.5934578679705826, 0.6321154263166202, 0.604190069167829, 0.6215277900380626, 0.6350854053005921, 0.5887227248992934, 0.6163991882503066, 0.6338788586549414, 0.6327512411494354, 0.6165308713382622, 
# 0.6328432880111924, 0.5962425281912822, 0.6318115018919988, 0.6074945241078687, 0.6375958733380773, 0.5946490874423888, 0.6042204242066529, 0.6373397703587451, 0.6204121962376363, 0.6186348052147899, 0.6070735062206669, 0.5891403578283665, 0.6282940648588822, 0.6364597480716423, 0.6356030225757152, 0.6012380141574093, 0.6411352839525453, 0.6302434127822859, 0.6167472576894701, 0.6310181273895658, 0.6196445704606723, 0.6297512220299244, 0.628418297911296, 0.6275352549939951, 0.6350351419781346, 0.6125138417598729, 0.6065840069580114, 0.6295411894740471, 0.601877950361595, 0.603233064021794, 0.6201436732249788, 0.6104591754068351, 0.6328457404092256, 0.6147262897799801, 0.6027229838306534, 0.6251955887132996, 0.6158985639765971, 0.6237907135994614, 0.630885415106887, 0.6324955806410603, 0.6216607279379665, 0.6203677016570097, 0.6267402586035018, 0.6214887932091482, 0.6120262164988629, 0.6389234267105981, 0.6158905196053082, 0.6356959630182047, 0.6260163145885305, 0.6362482515366068, 0.6481417483015961, 0.6085404409320355, 0.6356235022654537, 0.5733055372689699, 0.5968343452849612, 0.6059156792077336, 0.6179208903399114, 0.6270687159709947, 0.6276536538466615, 0.6022722356193406, 0.6083305421477821, 0.6394605051968535, 0.6263538378197397, 0.6134088598745457, 0.6293064300522939, 0.6048225211069782, 0.6186443911225283, 0.628332731952054, 0.5928332902417137, 0.635582526772462, 0.6103052741283741, 0.6166254115951662, 0.6077866093013992, 0.605519637872749, 0.5955791695475774, 0.6383787741613847, 0.6233978934010251, 0.6072510645278473]

# # Use the bootstrapped results to assess the confidence intervals of the results of the linear model. (only on the selected attributes)
confidence_interval = np.percentile(list_r2s, [2.5, 97.5])
print(mean_r2)
print(confidence_interval) # [0.58983975 0.64356827]

